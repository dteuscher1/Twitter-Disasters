---
title: "Disaster Tweets: NLP Approaches"
output: 
  html_document:
    toc: TRUE
    theme: united
    number_sections: TRUE
    code_folding: show
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, message=FALSE, warning=FALSE}
library(stringr)
library(sentimentr)
library(tidyverse)
library(tidytext)
library(caret)
library(text2vec)
library(glmnet)
library(randomForest)
library(scales)
library(DataExplorer)
```

# Exploratory Data Analysis

The plots below show the percentage of missing values for each variable in the training set and the test set. The proportion of missing values is practically the same between the training and test set. There are a lot of tweets that don't have a location at all and almost all of the tweets have keywords. 
```{r, message=FALSE, warning=FALSE}
# Read in training data
twitter <- read_csv("train.csv")
twitter_test <- read_csv('test.csv')

plot_missing(twitter)
plot_missing(twitter_test)
```

The plot below shows that more of the tweets are not about real disasters in the training set. 
```{r}
ggplot(twitter, aes(x = as.factor(target))) + 
  geom_bar(fill = "skyblue") +
  labs(x = "Disaster", y = "Count") +
  scale_x_discrete(labels = c("No", "Yes")) +
  coord_flip() +
  theme_bw()
```


The plot below shows which words tend to be in disaster tweets more often and which words tend to be in non-disaster tweets more often. 
```{r, message=FALSE, warning=FALSE}
tidy_twit <- twitter %>%
  mutate(text = gsub("http(.*)", "http", text)) %>%
  unnest_tokens(word, text) %>%
  anti_join(stop_words) %>%
  mutate(word = str_extract(word, '[\\da-z\']+'))

frequency <- tidy_twit %>%
  group_by(target) %>%
  count(word) %>%
  mutate(proportion = n/sum(n)) %>%
  select(-n) %>%
  spread(target, proportion) 

# Expect warning messages about missing values
ggplot(frequency, aes(x = `0`, y = `1`, color = abs(`1` - `0`))) +
  geom_abline(color = "gray40", lty = 2) +
  geom_jitter(alpha = 0.1, size = 2.5, width = 0.3, height = 0.3) +
  geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +
  scale_x_log10(labels = percent_format()) +
  scale_y_log10(labels = percent_format()) +
  scale_color_gradient(limits = c(0, 0.001), low = "darkslategray4", high = "gray75") +
  theme(legend.position="none") +
  labs(title="Word Frequency (in Proportions)\nBetween Tweet Message Types", y="Disaster Tweet", x="Irrelevant Tweet")

```

# Preprocessing

## Feature Engineering

We created variables to represent any useful patterns found within texts that may indicate whether a given tweet is about a real disaster. Specifically, we counted the number of urls in the tweet, counted the number of punctuation marks, the number of `@`, the number of hashtags, the length of the tweet, the number of capital letters and the proportion of capital letters out of total characters, the amount of numbers included in the tweets, the number of words in the tweets, and the average sentiment for each tweet. 

```{r}
custom_features <- function(twitter) {
  # Wrapper for all custom variables
  # Useful for adding custom features for train and test sets separately
  # Args:
  #   twitter: data.frame with twitter dataset from Kaggle nlp challenge
  # Returns:
  #   original twitter dataset including custom variable columns
  
  twitter <- twitter %>%
    mutate(url_count = str_count(text, "http[\\S]+"),
           text = str_replace_all(text, "http[\\S]+", "http"), # remove URLs
           punct_count = str_count(text, "[.!?,\"'-]"),
           handles_count = str_count(text, "[@]"),
           hashtag_count = str_count(twitter$text, "[#]"),
           char_count = nchar(twitter$text), # tweet length
           capital_count = str_count(twitter$text, "[A-Z]"),
           capital_prop = capital_count/char_count,
           number_count = str_count(twitter$text, "[0-9]")
           )
  
  # Add message tone variable
  sentiment_df <- sentiment_by(get_sentences(twitter$text))
  twitter$tone <- sentiment_df$ave_sentiment
  
  # Add word count
  twitter$word <- sentiment_df$word_count
  
  return(twitter)
}
```

The function created to calculate features from the tweets was used on the training and test sets. 
```{r, message=FALSE}
twitter <- custom_features(twitter)
twitter_test <- custom_features(twitter_test)
```

The tone of the tweets is on average slightly negative, which makes sense since disasters are a negative event and the words that are frequently associated with a disaster (whether real or not) have a negative connotation. 
```{r, message=FALSE}
ggplot(twitter, aes(x = tone)) +
  geom_histogram(fill = "firebrick", bins = 15) +
  geom_vline(xintercept = median(twitter$tone), color = "black") +
  theme_bw()

```

It appears that tweets that are about real disasters have more characters than tweets that are not about disasters, which isn't surprising since most tweets about actual disasters try to provide as much information as possible. 
```{r}
ggplot(twitter, aes(x = as.factor(target), y = char_count)) + 
  geom_boxplot(alpha = .3) +
  geom_jitter(alpha = .2, color = "steelblue") +
  labs(x = "Disaster", y = "Character Count") +
  scale_x_discrete(labels = c("No", "Yes"))
```

## Converting Text to Usable Predictors (Term Document Frequency)
 This function creates an iterator that can be used to create a vocabulary, which is needed to get a term document matrix 
```{r}
text2vec_iterator <- function(twitter) {
  # Args:
  #   twitter: data.frame with twitter dataset from Kaggle nlp challenge
  # Returns:
  #   iterator object to create a vocabulary and vector space 
  
  prep_fun <- tolower
  tok_fun <- word_tokenizer
  iterator <- itoken(twitter$text,
                       preprocessor = tolower,
                       tokenizer = word_tokenizer,
                       ids = twitter$id,
                       progressbar = TRUE)
  return(iterator)
}
```


The text from the training and test set is used to create a vector space with more words than if we only used the training set. Word vectors generally improve with more data, so we included both when creating the word vector. Since we aren't fitting a model, this is beneficial to use because the corpus is larger and the word vector can be better placed in the vector space.

While the vocabulary is built and pruned using the training and test set, the document term matrix is created separately, one for the training set and one for the test set.
```{r}
set.seed(2020)

# use twit.1 to create vector space with words from entire corpus
twit.1 <- tibble(id = c(twitter$id, twitter_test$id),
                 text = c(twitter$text, twitter_test$text))

twit_train <- text2vec_iterator(twit.1)
vocab <- create_vocabulary(twit_train, stopwords = stop_words$word)
pruned_vocab <- prune_vocabulary(vocab, 
                                term_count_min = 10, 
                                doc_proportion_max = 0.5,
                                doc_proportion_min = 0.001)

# Create vector space for given vocabulary set
vectorizer <- vocab_vectorizer(pruned_vocab)

# Create document-term matrix for training data
twit_train <- text2vec_iterator(twitter)
dtm_train <- create_dtm(twit_train, vectorizer)

# ...for test data
twit_test <- text2vec_iterator(twitter_test)
dtm_test <- create_dtm(twit_test, vectorizer)
```

## Combine Custom and Term-Document-Frequency Variables

The document term matrix is combined with the custom variables that were calculated by the `custom_features()` function. 
```{r}
allvars_train <- twitter %>%
  select(-c(id:target)) %>%
  as.matrix() %>%
  cbind(dtm_train)
allvars_test <- twitter_test %>%
  select(-c(id:text)) %>%
  as.matrix() %>%
  cbind(dtm_test)
```

# Modeling

A number of different models were attempted to find the best model. A few of these attempts are shown here below.  

## Logistic Regression

### Cross-validation to Find Best Classification Threshold Value

A logistic regression model with an L1 penalty was fit with a 10 fold cross validation. Multiple different cutoffs were used to determine what cutoff value minimized the misclassification rate since the competition is focused on the accuracy of the model. There may be an improvement in the model performance if the mean F-score was calculated instead. 

```{r}
#Note: cross-validation assessed on only document-term matrix
set.seed(2020)
test.set <- sample( 1:nrow(twitter), 0.1*nrow(twitter))
twit.train <- twitter[-test.set,]
twit.test <- twitter[test.set,]

cv_train <- text2vec_iterator(twit.train)
cv_vocab <- create_vocabulary(cv_train)
cv_vectorizer <- vocab_vectorizer(cv_vocab)

# Create document-term matrix for training data
dtm_train <- create_dtm(cv_train, cv_vectorizer)

# Create document-term matrix for test data
cv_test <- text2vec_iterator(twit.test)
dtm_test <- create_dtm(cv_test, cv_vectorizer)

glmnet.classifier.cv <- cv.glmnet(x = dtm_train,
                                   y = as.factor(twit.train$target),
                                   family = "binomial",
                                   alpha = 1, 
                                   type.measure = "auc",
                                   nfolds = 10, 
                                   thresh = 1e-3,
                                   maxit = 1e3)

cutoffs <- seq(.3, .7, by=0.01)
cv.perc <- sapply(cutoffs, function(cutoff) {
    y_hat <- predict(glmnet.classifier.cv, dtm_test, type="response")[,1] > cutoff
    mean(y_hat == twit.test$target)
  })

cv_cutoff <- cutoffs[which.max(cv.perc)]
paste('Best cutoff proportion:', cv_cutoff)
```

### Prediction

The predictions for the logistic regression model are obtained here. The probabilities are extracted and then the classification is determined by the cutoff value that was previously determined. 

```{r}
glmnet.classifier.log <- cv.glmnet(x = allvars_train,
                                   y = as.factor(twitter$target),
                                   family = "binomial",
                                   alpha = 1, 
                                   type.measure = "auc",
                                   nfolds = 10, 
                                   thresh = 1e-3,
                                   maxit = 1e3)
plot(glmnet.classifier.log)
print(paste("max AUC =", round(max(glmnet.classifier.log$cvm), 4)))

# Prediction
preds.log <- predict(glmnet.classifier.log, allvars_test,
                     type="response")[,1]

#probability for voting ensemble
prob.log <- predict(glmnet.classifier.log, allvars_test, type="response")[,1]

preds.log <- as.integer(preds.log > cv_cutoff)
preds.log.out <- tibble(id=twitter_test$id, target=preds.log)
write_csv(preds.log.out, 'preds_log_out.csv')
```

## Naive Bayes

The Naive Bayes model did the worst out of all of the models that were attempted and wasn't used at all. 

```{r}
# Create vector of the response variable
y <- factor(ifelse(twitter$target == 1, "Yes", "No"))

# Specifies the type of cross validation and to return AUC, sensitivity, and specificity
myControl <- trainControl(
  method="none",
  classProbs = TRUE,
  summaryFunction = twoClassSummary
)

# Creates a grid to test different values of hyperparameters
grid <- expand.grid(laplace=0, usekernel=TRUE, adjust=1)

# Fit of the Naive Bayes model
nb.model <- train(
  x=as.matrix(allvars_train),
  y=y,
  method = "naive_bayes",
  trControl = myControl,
  tuneGrid = grid,
  metric="ROC"
)

# Voting Ensemble probs
prob.nb <- predict(nb.model, as.matrix(allvars_test), type='prob')[,2]

# Best ROC: Laplace 0, Bandwidth 1, usekernel TRUE
preds.nb <- predict(nb.model, as.matrix(allvars_test))
preds.nb <- as.numeric( as.character(preds.nb)=='Yes')

preds.nb.out <- tibble(id=twitter_test$id, target=preds.nb)
#write_csv(preds.nb.out, 'preds_nb_out.csv')
```

## Random Forest

The random forest model was okay, but could be improved as well. There wasn't a lot of effort that went into tuning the model. 

```{r}
# Random Forest Model
twitter.rf <- randomForest(x=as.matrix(allvars_train),
                           y=as.factor(twitter$target),
                           mtry=5,
                           ntree=5,
                           importance=TRUE)

# Voting ensemble proportions
prob.rf <- predict(twitter.rf, newdata=as.matrix(allvars_test), type='prob')[,2]

preds.rf <- predict(twitter.rf, newdata=as.matrix(allvars_test))
preds.rf.out <- tibble(id=twitter_test$id, target=preds.rf)
#write_csv(preds.rf.out, 'preds_rf_out.csv')
```


# Ensemble

An ensemble model was used to see if predictions would improve where we used a weighting voting ensemble. We initially wanted to use three models, but the Naive Bayes model performed poorly and the decision was made to only use the logistic regression and the random forest. The ensemble did worse than the logistic regression model by itself and it is likely because there were only 2 models that were included in the ensemble. 

## Weighted Voting Ensemble

Layout:
- 2 * Regularized Logistic Regression
- 1 * Random Forest
- 0 * Naive Bayes

```{r}
combined.prob <- (2*prob.log + prob.rf)/3
combined.out <- tibble(id=twitter_test$id, target=as.numeric(combined.prob > .5))
#write_csv(combined.out, 'combined_out.csv')
```


# Conclusion

The best model, the logistic regression model with a L1 penalizer, gave a mean F-1 score of .79650, which is approximately the 51st percentile of the competition. There are a number of things that can be improved in my opinion. First, I think some of the models can be tuned and then other models, such as gbm and xgboost models could be attempted. Second, with improved models, an ensemble is likely to be better than the current one that is based on two models only. Third, I think that more experience and research with NLP techniques and algorithms would help us to improve the approach to his problem. This is the first exposure I have had with NLP, so I still have a number of things to learn in order to improve models. 


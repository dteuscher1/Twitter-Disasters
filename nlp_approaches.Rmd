---
title: "Disaster Tweets: NLP Approaches"
output: html_document
---

<!--
TO DO:
* Create ensemble for multiple methods
** Create bagging ensemble (one model at a time)
** See https://www.analyticsvidhya.com/blog/2018/06/comprehensive-guide-for-ensemble-models/
* Play with model parameters
* Normalize variables
-->

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(stringr)
library(sentimentr)
library(tidyverse)
library(tidytext)
library(caret)
library(text2vec)
library(glmnet)
library(randomForest)
```

# Preprocessing

## Feature Engineering

To begin with, we created variables to represent any useful patterns found within texts that may indicate whether a given tweet is about a real disaster.

```{r}
custom_features <- function(twitter) {
  # Wrapper for all custom variables
  # Useful for adding custom features for train and test sets separately
  # Args:
  #   twitter: data.frame with twitter dataset from Kaggle nlp challenge
  # Returns:
  #   original twitter dataset including custom variable columns
  
  twitter <- twitter %>%
    mutate(url_count = str_count(text, "http[\\S]+"),
           text = str_replace_all(text, "http[\\S]+", "http"), # remove URLs
           punct_count = str_count(text, "[.!?,\"'-]"),
           handles_count = str_count(text, "[@]"),
           hashtag_count = str_count(twitter$text, "[#]"),
           char_count = nchar(twitter$text), # tweet length
           capital_count = str_count(twitter$text, "[A-Z]"),
           capital_prop = capital_count/char_count,
           number_count = str_count(twitter$text, "[0-9]")
           )
  
  # Add message tone variable
  sentiment_df <- sentiment_by(get_sentences(twitter$text))
  twitter$tone <- sentiment_df$ave_sentiment
  
  # Add word count
  twitter$word <- sentiment_df$word_count
  
  return(twitter)
}
```

```{r, message=FALSE}
# Read in training data
twitter <- read_csv("train.csv")
twitter_test <- read_csv('test.csv')

twitter <- custom_features(twitter)
twitter_test <- custom_features(twitter_test)
```

## Converting Text to Usable Predictors (Term Document Frequency)

```{r}
text2vec_iterator <- function(twitter) {
  # Args:
  #   twitter: data.frame with twitter dataset from Kaggle nlp challenge
  # Returns:
  #   iterator object to create a vocabulary and vector space 
  
  prep_fun <- tolower
  tok_fun <- word_tokenizer
  iterator <- itoken(twitter$text,
                       preprocessor = tolower,
                       tokenizer = word_tokenizer,
                       ids = twitter$id,
                       progressbar = TRUE)
  return(iterator)
}
```

```{r}
set.seed(2020)

# use twit.1 to create vector space with words from entire corpus
twit.1 <- tibble(id = c(twitter$id, twitter_test$id),
                 text = c(twitter$text, twitter_test$text))

twit_train <- text2vec_iterator(twit.1)
vocab <- create_vocabulary(twit_train, stopwords = stop_words$word)
pruned_vocab <- prune_vocabulary(vocab, 
                                term_count_min = 10, 
                                doc_proportion_max = 0.5,
                                doc_proportion_min = 0.001)

# Create vector space for given vocabulary set
vectorizer <- vocab_vectorizer(pruned_vocab)

# Create document-term matrix for training data
twit_train <- text2vec_iterator(twitter)
dtm_train <- create_dtm(twit_train, vectorizer)

# ...for test data
twit_test <- text2vec_iterator(twitter_test)
dtm_test <- create_dtm(twit_test, vectorizer)
```

## Combine Custom and Term-Document-Frequency Variables

```{r}
allvars_train <- twitter %>%
  select(-c(id:target)) %>%
  as.matrix() %>%
  cbind(dtm_train)
allvars_test <- twitter_test %>%
  select(-c(id:text)) %>%
  as.matrix() %>%
  cbind(dtm_test)
```

# Modeling

## Logistic Regression

### Cross-validation to Find Best Classification Threshold Value

```{r}
#Note: cross-validation assessed on only document-term matrix
set.seed(2020)
test.set <- sample( 1:nrow(twitter), 0.1*nrow(twitter))
twit.train <- twitter[-test.set,]
twit.test <- twitter[test.set,]

cv_train <- text2vec_iterator(twit.train)
cv_vocab <- create_vocabulary(cv_train)
cv_vectorizer <- vocab_vectorizer(cv_vocab)

# Create document-term matrix for training data
dtm_train <- create_dtm(cv_train, cv_vectorizer)

# Create document-term matrix for test data
cv_test <- text2vec_iterator(twit.test)
dtm_test <- create_dtm(cv_test, cv_vectorizer)

glmnet.classifier.cv <- cv.glmnet(x = dtm_train,
                                   y = as.factor(twit.train$target),
                                   family = "binomial",
                                   alpha = 1, 
                                   type.measure = "auc",
                                   nfolds = 10, 
                                   thresh = 1e-3,
                                   maxit = 1e3)

cutoffs <- seq(.3, .7, by=0.01)
cv.perc <- sapply(cutoffs, function(cutoff) {
    y_hat <- predict(glmnet.classifier.cv, dtm_test, type="response")[,1] > cutoff
    mean(y_hat == twit.test$target)
  })

cv_cutoff <- cutoffs[which.max(cv.perc)]
paste('Best cutoff proportion:', cv_cutoff)
```

### Prediction

```{r}
glmnet.classifier.log <- cv.glmnet(x = allvars_train,
                                   y = as.factor(twitter$target),
                                   family = "binomial",
                                   alpha = 1, 
                                   type.measure = "auc",
                                   nfolds = 10, 
                                   thresh = 1e-3,
                                   maxit = 1e3)
plot(glmnet.classifier.log)
print(paste("max AUC =", round(max(glmnet.classifier.log$cvm), 4)))

# Prediction
preds.log <- predict(glmnet.classifier.log, allvars_test,
                     type="response")[,1]
preds.log <- as.integer(preds.log > cv_cutoff)
preds.log.out <- tibble(id=twitter_test$id, target=preds.log)
write_csv(preds.log.out, 'preds_log_out.csv')
```

# Naive Bayes

```{r}
# Create vector of the response variable
y <- factor(ifelse(twitter$target == 1, "Yes", "No"))

# Specifies the type of cross validation and to return AUC, sensitivity, and specificity
myControl <- trainControl(
  method="none",
  classProbs = TRUE,
  summaryFunction = twoClassSummary
)

# Creates a grid to test different values of hyperparameters
grid <- expand.grid(laplace=0, usekernel=TRUE, adjust=1)

# Fit of the Naive Bayes model
nb.model <- train(
  x=as.matrix(allvars_train),
  y=y,
  method = "naive_bayes",
  trControl = myControl,
  tuneGrid = grid,
  metric="ROC"
)

summary(nb.model)

# Best ROC: Laplace 0, Bandwidth 1, usekernel TRUE
preds.nb <- predict(nb.model, as.matrix(allvars_test))
preds.nb <- as.numeric( as.character(preds.nb)=='Yes')

preds.nb.out <- tibble(id=twitter_test$id, target=preds.nb)
write_csv(preds.nb.out, 'preds_nb_out.csv')
```

# Random Forest

```{r}
# Random Forest Model
twitter.rf <- randomForest(x=as.matrix(allvars_train),
                           y=as.factor(twitter$target),
                           mtry=5,
                           ntree=5,
                           importance=TRUE)

# RF plots we did with Heaton, but I forgot what they mean lol
plot(twitter.rf)
varImpPlot(twitter.rf)

preds.rf <- predict(twitter.rf, newdata=as.matrix(allvars_test))
preds.rf.out <- tibble(id=twitter_test$id, target=preds.rf)
write_csv(preds.rf.out, 'preds_rf_out.csv')
```

## Support Vector Machine
<!--NEED TO VERIFY IT WORKS AND PERFORMANCE-->

```{r}
# Making the target variable a factor
twitter %>% mutate(target = if_else(target=='1', 'Y', 'N'))

# Re-ordering columns. Unsure if necessary
twitter <- twitter[, c(1:4, 6:12, 5)]

svmFit <- train(x=allvars_train,
                y=twitter$target,
                data = twitter, 
                method = "svmRadial", 
                trControl = fitControl, 
                preProc = c("center", "scale"),
                tuneLength = 4,
                metric = "ROC")
svmFit 
```

## Principal Component Analysis (PCA)

The code below may be used to reduce the number of variables in both the train and test sets. The resulting transformation was recently tested on the training dataset with 1500+ variables, and the reduced dimensionality was n x 967. However, classification accuracy for regularized logistic regression was 4 percent points lower than classification without dimensionality reduction. Regardless, this may be useful for speeding up computation for SVM.

```{r}
pr.out <- prcomp(allvars_train, scale=TRUE)
pr.var <- pr.out$sdev ^2
pve <- pr.var/sum(pr.var)

plot(pve, xlab="Principal Component", 
     ylab="Proportion of Variance Explained", 
     type='b')
plot(cumsum(pve), xlab="Principal Component", 
     ylab="Cumulative Proportion of Variance Explained ",
     ylim=c(0,1), type='b')

# Find a reduced number of variables that still explain 90% of the variance in the data 
var_explained <- .9
ncomp <- sum(cumsum(pve) < var_explained) + 1
pca_matrix <- pr.out$rotation[,1:ncomp]

# Reduce number of variables of datasets to simplify computation and potentially reduce overfitting
# allvars_train <- allvars_train %*% pca_matrix
# allvars_test <- allvars_test %*% pca_matrix
```


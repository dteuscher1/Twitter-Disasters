---
title: "Disaster Tweets: NLP Approaches"
output: html_document
---

<!--
TO DO:
* Use custom features and tdf for classification
* Create ensemble for multiple methods
** Create bagging ensemble (one model at a time)
** See https://www.analyticsvidhya.com/blog/2018/06/comprehensive-guide-for-ensemble-models/
* TDF: remove stop-words
* Play with model parameters
-->

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(stringr)
library(sentimentr)
library(tidyverse)
library(tidytext)
library(caret)
library(text2vec)
library(glmnet)
```

# Preprocessing

## Feature Engineering

To begin with, we created variables to represent any useful patterns found within texts that may indicate whether a given tweet is about a real disaster.

```{r}
custom_features <- function(twitter) {
  # Wrapper for all custom variables
  # Useful for adding custom features for train and test sets separately
  # Args:
  #   twitter: data.frame with twitter dataset from Kaggle nlp challenge
  # Returns:
  #   original twitter dataset including custom variable columns
  
  twitter <- twitter %>%
    mutate(url_count = str_count(text, "http[\\S]+"),
           text = str_replace_all(text, "http[\\S]+", "http"), # remove URLs
           punct_count = str_count(text, "[.!?,\"'-]"),
           handles_count = str_count(text, "[@]"),
           hashtag_count = str_count(twitter$text, "[#]"),
           char_count = nchar(twitter$text), # tweet length
           capital_count = str_count(twitter$text, "[A-Z]"),
           capital_prop = capital_count/char_count,
           number_count = str_count(twitter$text, "[0-9]")
           )
  
  # Add message tone variable
  sentiment_df <- sentiment_by(get_sentences(twitter$text))
  twitter$tone <- sentiment_df$ave_sentiment
  
  # Add word count
  twitter$word <- sentiment_df$word_count
  
  return(twitter)
}
```

```{r, message=FALSE}
# Read in training data
twitter <- read_csv("train.csv")
twitter_test <- read_csv('test.csv')

twitter <- custom_features(twitter)
twitter_test <- custom_features(twitter_test)
```

## Converting Text to Usable Predictors (Term Document Frequency)

```{r}
text2vec_iterator <- function(twitter) {
  # Args:
  #   twitter: data.frame with twitter dataset from Kaggle nlp challenge
  # Returns:
  #   iterator object to create a vocabulary and vector space 
  
  prep_fun <- tolower
  tok_fun <- word_tokenizer
  iterator <- itoken(twitter$text,
                       preprocessor = tolower,
                       tokenizer = word_tokenizer,
                       ids = twitter$id,
                       progressbar = TRUE)
  return(iterator)
}
```

```{r}
set.seed(2020)

# use twit.1 to create vector space with words from entire corpus
twit.1 <- tibble(id = c(twitter$id, twitter_test$id),
                 text = c(twitter$text, twitter_test$text))

twit_train <- text2vec_iterator(twit.1)
vocab <- create_vocabulary(twit_train, stopwords = stop_words$word)
pruned_vocab <- prune_vocabulary(vocab, 
                                term_count_min = 10, 
                                doc_proportion_max = 0.5,
                                doc_proportion_min = 0.001)

# Create vector space for given vocabulary set
vectorizer <- vocab_vectorizer(pruned_vocab)

# Create document-term matrix for training data
twit_train <- text2vec_iterator(twitter)
dtm_train <- create_dtm(twit_train, vectorizer)

# ...for test data
twit_test <- text2vec_iterator(twitter_test)
dtm_test <- create_dtm(twit_test, vectorizer)
```

## Combine Custom and Term-Document-Frequency Variables

```{r}
allvars_train <- twitter %>%
  select(-c(id:target)) %>%
  as.matrix() %>%
  cbind(dtm_train)
allvars_test <- twitter_test %>%
  select(-c(id:text)) %>%
  as.matrix() %>%
  cbind(dtm_test)
```

# Modeling

## Logistic Regression

### Cross-validation

```{r}
#Note: cross-validation assessed on only document-term matrix
set.seed(2020)
test.set <- sample( 1:nrow(twitter), 0.1*nrow(twitter))
twit.train <- twitter[-test.set,]
twit.test <- twitter[test.set,]

cv_train <- text2vec_iterator(twit.train)
cv_vocab <- create_vocabulary(cv_train)
cv_vectorizer <- vocab_vectorizer(cv_vocab)

# Create document-term matrix for training data
dtm_train <- create_dtm(cv_train, cv_vectorizer)

# Create document-term matrix for test data
cv_test <- text2vec_iterator(twit.test)
dtm_test <- create_dtm(cv_test, cv_vectorizer)

glmnet.classifier.cv <- cv.glmnet(x = dtm_train,
                                   y = as.factor(twit.train$target),
                                   family = "binomial",
                                   alpha = 1, 
                                   type.measure = "auc",
                                   nfolds = 10, 
                                   thresh = 1e-3,
                                   maxit = 1e3)

cutoffs <- seq(.3, .7, by=0.01)
cv.perc <- sapply(cutoffs, function(cutoff) {
    y_hat <- predict(glmnet.classifier.cv, dtm_test, type="response")[,1] > cutoff
    mean(y_hat == twit.test$target)
  })
cv_cutoff <- cutoffs[which.max(cv.perc)]
paste('Best cutoff proportion:', cv_cutoff)
```

### Prediction

```{r}
glmnet.classifier.log <- cv.glmnet(x = allvars_train,
                                   y = as.factor(twitter$target),
                                   family = "binomial",
                                   alpha = 1, 
                                   type.measure = "auc",
                                   nfolds = 10, 
                                   thresh = 1e-3,
                                   maxit = 1e3)
plot(glmnet.classifier.log)
print(paste("max AUC =", round(max(glmnet.classifier.log$cvm), 4)))

# Prediction
preds.log <- predict(glmnet.classifier.log, allvars_test,
                     type="response")[,1]
preds.log.out <- tibble(id=twitter_test$id, target=as.integer(preds.log > cv_cutoff))
write_csv(preds.log.out, 'preds_log_out.csv')
```

# Support Vector Machine
<!--This is as far as I have gotten so far with merging files (Skyler)-->
<!--NEED TO VERIFY IT WORKS AND PERFORMANCE-->

```{r}
# filling missing values
twitter$keyword[is.na(twitter$keyword)] <- "None"
twitter$location[is.na(twitter$location)] <- "None"

# Making the target variable a factor
twitter %>% mutate(target = if_else(target=='1', 'Y', 'N'))

# Re-ordering columns. Unsure if necessary
twitter <- twitter[, c(1:4, 6:12, 5)]

svmFit <- train(target ~ . -id -text, 
                data = twitter, 
                method = "svmRadial", 
                trControl = fitControl, 
                preProc = c("center", "scale"),
                tuneLength = 4,
                metric = "ROC")
svmFit 
```

# Naive Bayes
<!--NEED TO VERIFY IT WORKS AND PERFORMANCE-->

```{r}
# Add indicator variables for keyword and location
twitter1 <- twitter %>%
  mutate(target = factor(ifelse(target == 1, "Yes", "No"), levels = c("No", "Yes")),
         keyword_ind = ifelse(is.na(keyword), 0, 1),
         location_ind = ifelse(is.na(location), 0, 1))

# Create data frame of predictor variables
x <- twitter1 %>% select(-id, -text, -target, -keyword, -location) %>% as.data.frame()
# Create vector of the response variable
y <- twitter1$target

# Specifies the type of cross validation and to return AUC, sensitivity, and specificity
myControl <- trainControl(
  method="cv",
  number = 5,
  classProbs = TRUE,
  summaryFunction = twoClassSummary
)

# Creates a grid to test different values of hyperparameters
grid <- expand.grid(laplace=seq(0,10, length = 5), usekernel=c(TRUE,FALSE), adjust=seq(1,10, length = 5))

# Fit of the Naive Bayes model
nb.model <- train(
  x=x,
  y=y,
  method = "naive_bayes",
  trControl = myControl,
  tuneGrid = grid,
  metric="ROC"
)

nb.model
summary(nb.model)

# Show a plot comparing the models with different hyperparameter values
plot(nb.model)
```

# Random Forest
<!--NEED TO VERIFY IT WORKS AND PERFORMANCE-->

```{r}
library(randomForest)

# Indicator for non-NA locations and keywords
twitter$keywordInd <- !is.na(twitter$keyword)
twitter$locationIng <- !is.na(twitter$location)

# I didn't use the first few columns (id, keyword, location, text)
twitter.clean <- twitter[,-c(1:4)]
twitter.clean$target <- as.factor(twitter.clean$target)

# Subsetting to creating training and testing sets
twitter.sub <- sample(nrow(twitter), round(0.9*nrow(twitter)))
twitter.train.use <- twitter.clean[twitter.sub,]
twitter.train.test <- twitter.clean[-twitter.sub,]

# Random Forest Model
twitter.rf <- randomForest(target~.,
                           data=twitter.train.use,
                           mtry=5,
                           ntree=800,
                           importance=TRUE)

# RF plots we did with Heaton, but I forgot what they mean lol
plot(twitter.rf)
varImpPlot(twitter.rf)

# Prediction Assessment (I got around .72)
twitter.train.test$predict <- predict(twitter.rf, newdata=twitter.train.test)
sum(twitter.train.test$target == twitter.train.test$predict) / nrow(twitter.train.test)
```


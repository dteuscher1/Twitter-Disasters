---
title: "Disaster Tweets: NLP Approaches"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, message=FALSE, warning=FALSE}
library(stringr)
library(sentimentr)
library(tidyverse)
library(tidytext)
library(caret)
library(text2vec)
library(glmnet)
library(randomForest)
library(scales)
library(DataExplorer)
```

# Exploratory Data Analysis

The plots below show the percentage of missing values for each variable in the training set and the test set. The proportion of missing values is practically the same between the training and test set. There are a lot of tweets that don't have a location at all and almost all of the tweets have keywords. 
```{r, message=FALSE, warning=FALSE}
# Read in training data
twitter <- read_csv("train.csv")
twitter_test <- read_csv('test.csv')

plot_missing(twitter)
plot_missing(twitter_test)
```


The plot below shows which words tend to be in disaster tweets more often and which words tend to be in non-disaster tweets more often. 
```{r, message=FALSE, warning=FALSE}
tidy_twit <- twitter %>%
  mutate(text = gsub("http(.*)", "http", text)) %>%
  unnest_tokens(word, text) %>%
  anti_join(stop_words) %>%
  mutate(word = str_extract(word, '[\\da-z\']+'))

frequency <- tidy_twit %>%
  group_by(target) %>%
  count(word) %>%
  mutate(proportion = n/sum(n)) %>%
  select(-n) %>%
  spread(target, proportion) 

# Expect warning messages about missing values
ggplot(frequency, aes(x = `0`, y = `1`, color = abs(`1` - `0`))) +
  geom_abline(color = "gray40", lty = 2) +
  geom_jitter(alpha = 0.1, size = 2.5, width = 0.3, height = 0.3) +
  geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +
  scale_x_log10(labels = percent_format()) +
  scale_y_log10(labels = percent_format()) +
  scale_color_gradient(limits = c(0, 0.001), low = "darkslategray4", high = "gray75") +
  theme(legend.position="none") +
  labs(title="Word Frequency (in Proportions)\nBetween Tweet Message Types", y="Disaster Tweet", x="Irrelevant Tweet")

```

# Preprocessing

## Feature Engineering

We created variables to represent any useful patterns found within texts that may indicate whether a given tweet is about a real disaster. Specifically, we counted the number of urls in the tweet, counted the number of punctuation marks, the number of `@`, the number of hashtags, the length of the tweet, the number of capital letters and the proportion of capital letters out of total characters, the amount of numbers included in the tweets, the number of words in the tweets, and the average sentiment for each tweet. 

```{r}
custom_features <- function(twitter) {
  # Wrapper for all custom variables
  # Useful for adding custom features for train and test sets separately
  # Args:
  #   twitter: data.frame with twitter dataset from Kaggle nlp challenge
  # Returns:
  #   original twitter dataset including custom variable columns
  
  twitter <- twitter %>%
    mutate(url_count = str_count(text, "http[\\S]+"),
           text = str_replace_all(text, "http[\\S]+", "http"), # remove URLs
           punct_count = str_count(text, "[.!?,\"'-]"),
           handles_count = str_count(text, "[@]"),
           hashtag_count = str_count(twitter$text, "[#]"),
           char_count = nchar(twitter$text), # tweet length
           capital_count = str_count(twitter$text, "[A-Z]"),
           capital_prop = capital_count/char_count,
           number_count = str_count(twitter$text, "[0-9]")
           )
  
  # Add message tone variable
  sentiment_df <- sentiment_by(get_sentences(twitter$text))
  twitter$tone <- sentiment_df$ave_sentiment
  
  # Add word count
  twitter$word <- sentiment_df$word_count
  
  return(twitter)
}
```

The function created to calculate features from the tweets was used on the training and test sets. 
```{r, message=FALSE}
twitter <- custom_features(twitter)
twitter_test <- custom_features(twitter_test)
```

## Converting Text to Usable Predictors (Term Document Frequency)
 This function creates an iterator that can be used to create a vocabulary, which is needed to get a term document matrix 
```{r}
text2vec_iterator <- function(twitter) {
  # Args:
  #   twitter: data.frame with twitter dataset from Kaggle nlp challenge
  # Returns:
  #   iterator object to create a vocabulary and vector space 
  
  prep_fun <- tolower
  tok_fun <- word_tokenizer
  iterator <- itoken(twitter$text,
                       preprocessor = tolower,
                       tokenizer = word_tokenizer,
                       ids = twitter$id,
                       progressbar = TRUE)
  return(iterator)
}
```


The text from the training and test set is used to create a vector space with more words than if we only used the training set. Word vectors generally improve with more data, so we included both when creating the word vector. Since we aren't fitting a model, this is beneficial to use because the corpus is larger and the word vector can be better placed in the vector space.

While the vocabulary is built and pruned using the training and test set, the document term matrix is created separately, one for the training set and one for the test set.
```{r}
set.seed(2020)

# use twit.1 to create vector space with words from entire corpus
twit.1 <- tibble(id = c(twitter$id, twitter_test$id),
                 text = c(twitter$text, twitter_test$text))

twit_train <- text2vec_iterator(twit.1)
vocab <- create_vocabulary(twit_train, stopwords = stop_words$word)
pruned_vocab <- prune_vocabulary(vocab, 
                                term_count_min = 10, 
                                doc_proportion_max = 0.5,
                                doc_proportion_min = 0.001)

# Create vector space for given vocabulary set
vectorizer <- vocab_vectorizer(pruned_vocab)

# Create document-term matrix for training data
twit_train <- text2vec_iterator(twitter)
dtm_train <- create_dtm(twit_train, vectorizer)

# ...for test data
twit_test <- text2vec_iterator(twitter_test)
dtm_test <- create_dtm(twit_test, vectorizer)
```

## Combine Custom and Term-Document-Frequency Variables

The document term matrix is combined with the custom variables that were calculated by the `custom_features()` function. 
```{r}
allvars_train <- twitter %>%
  select(-c(id:target)) %>%
  as.matrix() %>%
  cbind(dtm_train)
allvars_test <- twitter_test %>%
  select(-c(id:text)) %>%
  as.matrix() %>%
  cbind(dtm_test)
```

# Modeling

A number of different models were attempted to find the best model. A few of these attempts are shown here below.  

## Logistic Regression

### Cross-validation to Find Best Classification Threshold Value

```{r}
#Note: cross-validation assessed on only document-term matrix
set.seed(2020)
test.set <- sample( 1:nrow(twitter), 0.1*nrow(twitter))
twit.train <- twitter[-test.set,]
twit.test <- twitter[test.set,]

cv_train <- text2vec_iterator(twit.train)
cv_vocab <- create_vocabulary(cv_train)
cv_vectorizer <- vocab_vectorizer(cv_vocab)

# Create document-term matrix for training data
dtm_train <- create_dtm(cv_train, cv_vectorizer)

# Create document-term matrix for test data
cv_test <- text2vec_iterator(twit.test)
dtm_test <- create_dtm(cv_test, cv_vectorizer)

glmnet.classifier.cv <- cv.glmnet(x = dtm_train,
                                   y = as.factor(twit.train$target),
                                   family = "binomial",
                                   alpha = 1, 
                                   type.measure = "auc",
                                   nfolds = 10, 
                                   thresh = 1e-3,
                                   maxit = 1e3)

cutoffs <- seq(.3, .7, by=0.01)
cv.perc <- sapply(cutoffs, function(cutoff) {
    y_hat <- predict(glmnet.classifier.cv, dtm_test, type="response")[,1] > cutoff
    mean(y_hat == twit.test$target)
  })

cv_cutoff <- cutoffs[which.max(cv.perc)]
paste('Best cutoff proportion:', cv_cutoff)
```

### Prediction

```{r}
glmnet.classifier.log <- cv.glmnet(x = allvars_train,
                                   y = as.factor(twitter$target),
                                   family = "binomial",
                                   alpha = 1, 
                                   type.measure = "auc",
                                   nfolds = 10, 
                                   thresh = 1e-3,
                                   maxit = 1e3)
plot(glmnet.classifier.log)
print(paste("max AUC =", round(max(glmnet.classifier.log$cvm), 4)))

# Prediction
preds.log <- predict(glmnet.classifier.log, allvars_test,
                     type="response")[,1]

#probability for voting ensemble
prob.log <- predict(glmnet.classifier.log, allvars_test, type="response")[,1]

preds.log <- as.integer(preds.log > cv_cutoff)
preds.log.out <- tibble(id=twitter_test$id, target=preds.log)
write_csv(preds.log.out, 'preds_log_out.csv')
```

## Naive Bayes

```{r}
# Create vector of the response variable
y <- factor(ifelse(twitter$target == 1, "Yes", "No"))

# Specifies the type of cross validation and to return AUC, sensitivity, and specificity
myControl <- trainControl(
  method="none",
  classProbs = TRUE,
  summaryFunction = twoClassSummary
)

# Creates a grid to test different values of hyperparameters
grid <- expand.grid(laplace=0, usekernel=TRUE, adjust=1)

# Fit of the Naive Bayes model
nb.model <- train(
  x=as.matrix(allvars_train),
  y=y,
  method = "naive_bayes",
  trControl = myControl,
  tuneGrid = grid,
  metric="ROC"
)

summary(nb.model)

# Voting Ensemble probs
prob.nb <- predict(nb.model, as.matrix(allvars_test), type='prob')[,2]

# Best ROC: Laplace 0, Bandwidth 1, usekernel TRUE
preds.nb <- predict(nb.model, as.matrix(allvars_test))
preds.nb <- as.numeric( as.character(preds.nb)=='Yes')

preds.nb.out <- tibble(id=twitter_test$id, target=preds.nb)
write_csv(preds.nb.out, 'preds_nb_out.csv')
```

## Random Forest

```{r}
# Random Forest Model
twitter.rf <- randomForest(x=as.matrix(allvars_train),
                           y=as.factor(twitter$target),
                           mtry=5,
                           ntree=5,
                           importance=TRUE)

# RF plots we did with Heaton, but I forgot what they mean lol
plot(twitter.rf)
varImpPlot(twitter.rf)

# Voting ensemble proportions
prob.rf <- predict(twitter.rf, newdata=as.matrix(allvars_test), type='prob')[,2]

preds.rf <- predict(twitter.rf, newdata=as.matrix(allvars_test))
preds.rf.out <- tibble(id=twitter_test$id, target=preds.rf)
write_csv(preds.rf.out, 'preds_rf_out.csv')
```

## Support Vector Machine
<!--TAKES TOO LONG TO COMPUTE FOREGOING FOR NOW-->

```{r}
# Create vector of the response variable
y <- factor(ifelse(twitter$target == 1, "Yes", "No"))

fitControl <- trainControl(method = "none",
                           #number = 10,
                           classProbs = TRUE,
                           summaryFunction = twoClassSummary)

svmFit <- train(x=as.matrix(allvars_train),
                y=y,
                data = twitter, 
                method = "svmRadial", 
                trControl = fitControl,
                sigma=1,
                tau=1,
                preProc = c("center", "scale"),
                metric = "ROC")
svmFit 
```

## Principal Component Analysis (PCA)

The code below may be used to reduce the number of variables in both the train and test sets. The resulting transformation was recently tested on the training dataset with 1500+ variables, and the reduced dimensionality was n x 967. However, classification accuracy for regularized logistic regression was 4 percent points lower than classification without dimensionality reduction. Regardless, this may be useful for speeding up computation for SVM.

```{r}
pr.out <- prcomp(allvars_train, scale=TRUE)
pr.var <- pr.out$sdev ^2
pve <- pr.var/sum(pr.var)

plot(pve, xlab="Principal Component", 
     ylab="Proportion of Variance Explained", 
     type='b')
plot(cumsum(pve), xlab="Principal Component", 
     ylab="Cumulative Proportion of Variance Explained ",
     ylim=c(0,1), type='b')

# Find a reduced number of variables that still explain 90% of the variance in the data 
var_explained <- .9
ncomp <- sum(cumsum(pve) < var_explained) + 1
pca_matrix <- pr.out$rotation[,1:ncomp]

# Reduce number of variables of datasets to simplify computation and potentially reduce overfitting
# allvars_train <- allvars_train %*% pca_matrix
# allvars_test <- allvars_test %*% pca_matrix
```

# Ensemble

## Stacked Ensemble

Ensemble Layout:
- Naive Bayes
- Random Forest
- Regularized Logistic Regression

## Weighted Voting Ensemble

Layout:
- 2 * Regularized Logistic Regression
- 1 * Random Forest
- 0 * Naive Bayes

```{r}
combined.prob <- (2*prob.log + prob.rf)/3
combined.out <- tibble(id=twitter_test$id, target=as.numeric(combined.prob > .5))
write_csv(combined.out, 'combined_out.csv')
```

